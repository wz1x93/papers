"title": "2.5D Visual Sound",
"status": "Oral",
"site": "",
"track": "main",
"project": "http://vision.cs.utexas.edu/projects/2.5D_visual_sound/",
"github": "",
"pdf": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_2.5D_Visual_Sound_CVPR_2019_paper.pdf",
"youtube": "",
"author": "Ruohan Gao; Kristen Grauman",
"aff": "Facebook AI Research; The University of Texas at Austin",
"oa": "https://openaccess.thecvf.com/content_CVPR_2019/html/Gao_2.5D_Visual_Sound_CVPR_2019_paper.html",
"arxiv": ""

这篇论文名为《2.5D Visual Sound》，作者是 Ruohan Gao 和 Kristen Grauman。论文提出了一种将单声道音频转换为双声道音频的方法，通过利用视频中的视觉信息来实现这一目标。这种方法被称为“2.5D 视觉声音”，旨在通过视频帧中的空间线索来增强音频的三维感，从而为听众提供更丰富的感知体验。

### 研究背景与动机
- **双声道音频的重要性**：双声道音频能够为听众提供三维声音体验，使人能够感知声音的方向和位置。然而，双声道录音设备昂贵且难以获取，大多数消费级相机仅能录制单声道音频。
- **多模态感知**：人类通过视觉和听觉等多种感官来感知世界，两者相互协作以解释空间信号。例如，在嘈杂的派对中，人们可以凭借视觉线索定位声音来源。

### 研究方法
- **核心思想**：利用视频帧中的空间信息来指导单声道音频向双声道音频的转换。视频帧中包含了物体和场景的配置信息，这些信息可以用来推断声音的来源和位置。
- **网络架构**：作者设计了一个名为 MONO2BINAURAL 的深度卷积神经网络。该网络采用编码器-解码器结构，输入为混合的单声道音频及其对应的视频帧，输出为双声道音频。
- **训练数据**：由于缺乏包含双声道音频的大型公共视频数据集，作者收集了一个名为 FAIR-Play 的数据集，包含 5.2 小时的音乐表演视频，这些视频使用专业的双声道麦克风录制。
- **预测方法**：网络通过预测两个声道之间的差异信号来实现音频的空间化。具体来说，网络预测左声道和右声道的差值信号 \(x_D(t) = x_L(t) - x_R(t)\)，然后通过逆短时傅里叶变换（ISTFT）恢复出双声道音频。

### 实验与结果
- **数据集**：作者在四个具有挑战性的数据集上验证了他们的方法，包括 FAIR-Play、REC-STREET、YT-CLEAN 和 YT-MUSIC，涵盖了乐器演奏、街道场景、旅行和体育等多种声音来源。
- **性能评估**：使用 STFT 距离和包络距离两种指标来评估预测的双声道音频质量。结果表明，MONO2BINAURAL 方法在所有数据集上均优于基线方法，包括 Ambisonics 方法、仅使用音频的 Audio-Only 方法和简单的单声道复制方法。
- **用户研究**：通过用户研究进一步验证了预测双声道音频的三维感。参与者被要求比较不同方法生成的双声道音频与真实双声道音频的匹配程度。结果显示，MONO2BINAURAL 方法生成的音频在三维感上更接近真实双声道音频。
- **声音源定位**：通过遮挡视频帧中的某些区域并观察对预测双声道音频的影响，作者发现网络能够关注到声音源的位置，从而证明了其在声音源定位方面的有效性。
- **音频-视觉源分离**：作者还展示了预测的双声道音频可以用于音频-视觉源分离任务。通过将预测的双声道音频与视觉信息结合，网络能够更好地分离混合音频中的不同声音源，相比仅使用单声道音频的方法取得了显著的性能提升。

### 研究贡献
- 提出了一种利用视频帧将单声道音频转换为双声道音频的方法，并设计了相应的深度学习网络。
- 收集并公开了一个包含双声道音频的视频数据集 FAIR-Play，为相关研究提供了宝贵的资源。
- 展示了预测的双声道音频在音频-视觉源分离任务中的应用，证明了其作为一种更好的音频表示形式的潜力。

### 结论
论文提出了一种创新的方法，通过结合视觉信息将单声道音频转换为双声道音频，从而增强了音频的三维感。这种方法不仅在音频空间化方面取得了优异的性能，还为音频-视觉源分离任务提供了新的思路。未来的工作可能会进一步探索如何将物体定位和运动信息纳入模型，以进一步提升音频空间化的准确性和鲁棒性。
